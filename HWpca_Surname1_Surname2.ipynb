{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6347f1d2-5cb0-4032-b637-4730e9b9383a",
   "metadata": {},
   "source": [
    "# Computational Linear Algebra: PCA Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a905c56f-029b-4d04-9e68-739362350bd8",
   "metadata": {},
   "source": [
    "## Initialization:\n",
    "Fill the missing values in this text box and in the following code-cell.\n",
    "\n",
    "**Academic Year:** 2024/2025\n",
    "\n",
    "### Team Members (Alphabetical Order):\n",
    "1. Morales Condorpocco, Brian Facundo (346581);\n",
    "2. Spinnicchia, Andrea (344912)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2c068e99-0c8c-433f-be0b-f41534dc8a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "StudentID1 = 346581\n",
    "StudentID2 = 344912"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a24f22b-a92f-4898-b26d-e4ab6d49c3d9",
   "metadata": {},
   "source": [
    "## Starting Code-Cell \n",
    "### Attention: DO NOT CHANGE THE CODE INSIDE THE FOLLOWING CELL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84734de0-62ad-4c39-b4b4-886696d3a3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "############## DO NOT CHANGE THE CODE IN THIS CELL #################\n",
    "####################################################################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "var_entertainment_feat_types = ['Interests', 'Movies', 'Music']\n",
    "var_personal_feat_types = ['Finance', 'Phobias']\n",
    "fixed_feat_types = ['Personality', 'Health']\n",
    "\n",
    "label_types = ['Demographic']\n",
    "\n",
    "variables_by_type = {\n",
    "    'Demographics': ['Age', 'Height', 'Weight', 'Number of siblings', \n",
    "                     'Gender', 'Hand', 'Education', 'Only child', 'Home Town Type',\n",
    "                     'Home Type'],\n",
    "    'Finance': ['Finances', 'Shopping centres', 'Branded clothing', \n",
    "                'Entertainment spending', 'Spending on looks', \n",
    "                'Spending on gadgets', 'Spending on healthy eating'],\n",
    "    'Health': ['Smoking', 'Alcohol', 'Healthy eating'],\n",
    "    'Interests': ['History', 'Psychology', 'Politics', 'Mathematics', \n",
    "                  'Physics', 'Internet', 'PC', 'Economy Management', \n",
    "                  'Biology', 'Chemistry', 'Reading', 'Geography', \n",
    "                  'Foreign languages', 'Medicine', 'Law', 'Cars', \n",
    "                  'Art exhibitions', 'Religion', 'Countryside, outdoors', \n",
    "                  'Dancing', 'Musical instruments', 'Writing', 'Passive sport', \n",
    "                  'Active sport', 'Gardening', 'Celebrities', 'Shopping', \n",
    "                  'Science and technology', 'Theatre', 'Fun with friends', \n",
    "                  'Adrenaline sports', 'Pets'],\n",
    "    'Movies': ['Movies', 'Horror', 'Thriller', 'Comedy', 'Romantic', \n",
    "               'Sci-fi', 'War', 'Fantasy/Fairy tales', 'Animated', \n",
    "               'Documentary', 'Western', 'Action'],\n",
    "    'Music': ['Music', 'Slow songs or fast songs', 'Dance', 'Folk', \n",
    "              'Country', 'Classical music', 'Musical', 'Pop', 'Rock', \n",
    "              'Metal or Hardrock', 'Punk', 'Hiphop, Rap', 'Reggae, Ska', \n",
    "              'Swing, Jazz', 'Rock n roll', 'Alternative', 'Latino', \n",
    "              'Techno, Trance', 'Opera'],\n",
    "    'Personality': ['Daily events', 'Prioritising workload', \n",
    "                    'Writing notes', 'Workaholism', 'Thinking ahead', \n",
    "                    'Final judgement', 'Reliability', 'Keeping promises', \n",
    "                    'Loss of interest', 'Friends versus money', 'Funniness', \n",
    "                    'Fake', 'Criminal damage', 'Decision making', 'Elections', \n",
    "                    'Self-criticism', 'Judgment calls', 'Hypochondria', \n",
    "                    'Empathy', 'Eating to survive', 'Giving', \n",
    "                    'Compassion to animals', 'Borrowed stuff', \n",
    "                    'Loneliness', 'Cheating in school', 'Health', \n",
    "                    'Changing the past', 'God', 'Dreams', 'Charity', \n",
    "                    'Number of friends', 'Punctuality', 'Lying', 'Waiting', \n",
    "                    'New environment', 'Mood swings', 'Appearence and gestures', \n",
    "                    'Socializing', 'Achievements', 'Responding to a serious letter', \n",
    "                    'Children', 'Assertiveness', 'Getting angry', \n",
    "                    'Knowing the right people', 'Public speaking', \n",
    "                    'Unpopularity', 'Life struggles', 'Happiness in life', \n",
    "                    'Energy levels', 'Small - big dogs', 'Personality', \n",
    "                    'Finding lost valuables', 'Getting up', 'Interests or hobbies', \n",
    "                    \"Parents' advice\", 'Questionnaires or polls', 'Internet usage'],\n",
    "    'Phobias': ['Flying', 'Storm', 'Darkness', 'Heights', 'Spiders', 'Snakes', \n",
    "                'Rats', 'Ageing', 'Dangerous dogs', 'Fear of public speaking']\n",
    "}\n",
    "\n",
    "labels = variables_by_type['Demographics']\n",
    "\n",
    "try:\n",
    "    random_seed = min([StudentID1, StudentID2])\n",
    "except NameError:\n",
    "    random_seed = StudentID1\n",
    "\n",
    "def which_featgroups():\n",
    "    np.random.seed(random_seed)\n",
    "    these_entertainments = np.random.choice(var_entertainment_feat_types, 2, replace=False).tolist()\n",
    "    these_personal = np.random.choice(var_personal_feat_types, 1, replace=False).tolist()\n",
    "    these_types = fixed_feat_types + these_personal + these_entertainments\n",
    "    print('*** THESE ARE THE SELECTED TYPE OF VARIABLES:')\n",
    "    for k in these_types:\n",
    "        print(f'{k}')\n",
    "    print('*************************************')\n",
    "    return these_types\n",
    "\n",
    "def which_features(these_types):\n",
    "    np.random.seed(random_seed)\n",
    "    these_features = []\n",
    "    for type in these_types:\n",
    "        if type != 'Personality':\n",
    "            these_features += variables_by_type[type]\n",
    "        else:\n",
    "            these_features += np.random.choice(variables_by_type[type], \n",
    "                                               int(2 * (len(variables_by_type[type]) / 3)), \n",
    "                                               replace=False).tolist()\n",
    "    print('*** THESE ARE THE SELECTED FEATURES:')\n",
    "    for ft in these_features:\n",
    "        print(f'{ft}')\n",
    "    print('*************************************')\n",
    "    return these_features\n",
    "\n",
    "these_types = which_featgroups()\n",
    "these_features = which_features(these_types)\n",
    "\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090c9090-6065-4f25-bdc3-1b3cdad6c083",
   "metadata": {},
   "source": [
    "## Importing Modules\n",
    "\n",
    "In the following cell, import all the modules you think are necessary for doing the homework, **among the ones listed and used during the laboratories of the course**.\n",
    "No extra modules are allowed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "30ad703d-e6d8-4239-8222-7aa91fbda3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT IMPORT NUMPY\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1143d0-e6a8-43dd-8bf8-0363842bac60",
   "metadata": {},
   "source": [
    "## Exercise 1. Preparing the Dataset\n",
    "\n",
    "In the cells below, do the following operations:\n",
    "1. load the dataset \"_responses_hw.csv_\";\n",
    "2. create a working dataframe extracting from _responses_hw.csv_ the columns corresponding to the variables in _these_features_, and randomly selecting 2/3 of the rows. Let us call this dataframe _X_df_;\n",
    "3. analyze the obtained dataframe and performing cleansing/encoding operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b2d477-033c-40fd-b31e-f7c03ddb2954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATH TO THE responses_hw.csv FILE\n",
    "responses_path = \"responses_hw.csv\"\n",
    "\n",
    "# LOADING THE DATASET AS DATAFRAME\n",
    "responses_df = pd.read_csv(responses_path)\n",
    "\n",
    "# DISPLAY THE DATAFRAME\n",
    "display(responses_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd29876d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALID FEATURES IN THE DATAFRAME OBTAINED FROM THE VARIABLES IN these_features \n",
    "valid_features = [feature for feature in these_features if feature in responses_df.columns.values]\n",
    "if not valid_features:\n",
    "    raise ValueError(\"No valid features found in the dataset for these_features.\")\n",
    "\n",
    "# NEW DATAFRAME WITH THE COLUMNS CORRESPONDING TO THE VARIABLES IN these_features\n",
    "working_df = responses_df[valid_features]\n",
    "\n",
    "# RANDOMLY SELECT 2/3 OF THE ROWS\n",
    "X_df = working_df.sample(frac=2/3, random_state=0)\n",
    "\n",
    "# DISPLAY THE OBTAINED DATAFRAME\n",
    "display(X_df)\n",
    "X_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd55b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the DataFrame\n",
    "print(\"Summary of X_df:\")\n",
    "print(X_df.describe(include='all'))\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = X_df.isnull().sum()\n",
    "print(\"\\nMissing values in each column:\")\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93814eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanse/encode operations\n",
    "X_df = X_df.dropna()\n",
    "\n",
    "X_df['Alcohol'] = X_df['Alcohol'].map({'drink a lot':2, 'social drinker':1, 'never':0})\n",
    "X_df['Smoking'] = X_df['Smoking'].map({'never smoked':0, 'tried smoking':1, 'current smoker':3, 'former smoker':2})\n",
    "X_df['Punctuality'] = X_df['Punctuality'].map({'on time':1, 'early':0, 'late':1})\n",
    "\n",
    "# DISPLAY THE OBTAINED DATAFRAME\n",
    "display(X_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4058cb9-19fc-406c-8380-7e8053b43649",
   "metadata": {},
   "source": [
    "## Exercise 2. Analyzing the Variance and the PCs\n",
    "\n",
    "In the cells below, do the following operations:\n",
    "1. create two new dataframes from _X_df_ applying a StandardScaler and a MinMaxscaler. Call these new dataframes as _Xstd_df_ and _Xmm_df_, respectively;\n",
    "2. compute the variance of all the features in _X_df_, _Xstd_df_, and _Xmm_df_ and **comment the results**;\n",
    "3. compute all the $n$ Principal Components (PCs) for each dataset _X_df_, _Xstd_df_, and _Xmm_df_. Then, visualize the curves of the cumulative explained variances and **comment the results**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181358a6-6bb0-4f22-b395-34e3757fa16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_std = StandardScaler()\n",
    "scaler_std.fit(X_df.values)  # Fit the scaler on the X_df\n",
    "\n",
    "Xstd_df = pd.DataFrame(scaler_std.transform(X_df.values)  )\n",
    "\n",
    "scaler_mm = MinMaxScaler()\n",
    "scaler_mm.fit(X_df.values)  # Fit the scaler on the X_df\n",
    "\n",
    "Xmm_df = pd.DataFrame(scaler_mm.transform(X_df.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb99d072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute PCA and cumulative explained variance\n",
    "def compute_pca_and_plot(X, label):\n",
    "    pca = PCA()\n",
    "    pca.fit(X)\n",
    "\n",
    "    # Cumulative explained variance\n",
    "    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "    # Plot the cumulative explained variance\n",
    "    plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, label=label)\n",
    "    plt.xlabel('Number of Principal Components')\n",
    "    plt.ylabel('Variance')\n",
    "    plt.title('Cumulative explained variance')\n",
    "    plt.grid()\n",
    "\n",
    "    return cumulative_variance\n",
    "\n",
    "# Plot for all three datasets\n",
    "plt.figure(figsize=(10, 6))\n",
    "cum_var_X = compute_pca_and_plot(X_df, label='X_df')\n",
    "cum_var_Xstd = compute_pca_and_plot(Xstd_df, label='Xstd_df')\n",
    "cum_var_Xmm = compute_pca_and_plot(Xmm_df, label='Xmm_df')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c1771a-09b2-41bc-bc4c-7e3f6f530021",
   "metadata": {},
   "source": [
    "## Exercise 3. Dimensionality Reduction and PC Interpretation\n",
    "\n",
    "In the cells below, do the following operations:\n",
    "1. For each one of the two dataframes _Xstd_df_, and _Xmm_df_, compute a new PCA for performing a dimensionality reduction with respect to $m$ dimensions. The value of $m$ must be $$m = \\min\\{m', 5\\}\\,,$$ where $m'$ is the value required for obtaining $33\\%$ of the total variance.\n",
    "2. For both the cases, visualize all the PCs and give a name/interpretation to them. **Comment and motivate your interpretations**. If possible, **compare the differences among the results obtained** for _Xstd_df_ and _Xmm_df_.\n",
    "3. Perform the score graph for both the cases (_std_ and _mm_). If $m>3$, plot the score graph with respect to the first 3 PCs. All the **plots must show the names of the PCs on the axes** for better understanding the results.\n",
    "4. **Optional:** plot more score graphs, coloring the dots with respect to any label in the list _labels_ that you believe can be interesting. **Comment and analyze this optional plots**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c6c6d02-e58c-4db4-8872-d45c6eb9e4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to compute m and perform dimensionality reduction\n",
    "def reduce_to_m_dimensions(X, target_variance=0.33, max_components=5):\n",
    "    pca = PCA()\n",
    "    pca.fit(X)\n",
    "    \n",
    "    # Compute m'\n",
    "    cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    m_prime = np.searchsorted(cumulative_variance, target_variance) + 1  # +1 for index\n",
    "    \n",
    "    # Determine m\n",
    "    m = min(m_prime, max_components)\n",
    "    \n",
    "    # Dimensionality reduction\n",
    "    pca_reduction = PCA(n_components=m)\n",
    "    X_reduced = pca_reduction.fit_transform(X)\n",
    "    \n",
    "    return X_reduced, pca_reduction\n",
    "\n",
    "# Dimensionality reduction for Xstd_df and Xmm_df\n",
    "Xstd_reduced, pca_std = reduce_to_m_dimensions(Xstd_df)\n",
    "Xmm_reduced, pca_mm = reduce_to_m_dimensions(Xmm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfae5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_score_graphs(X):\n",
    "\n",
    "    # Fit both the pca objects on the data\n",
    "    pca_std.fit(X)\n",
    "\n",
    "    # transform both the data using the pca objects\n",
    "    Y_reduced = pca_std.transform(X)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.scatter(Y_reduced[:, 0], Y_reduced[:, 1])\n",
    "    plt.title('2D SCORE GRAPH')\n",
    "    plt.xlabel('PC1')\n",
    "    plt.ylabel('PC2')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(Y_reduced[:, 0], Y_reduced[:, 1], Y_reduced[:, 2])\n",
    "    plt.title('3D SCORE GRAPH')\n",
    "    ax.set_xlabel('PC1')\n",
    "    ax.set_ylabel('PC2')\n",
    "    ax.set_zlabel('PC3')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "plot_score_graphs(Xstd_reduced)\n",
    "plot_score_graphs(Xmm_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dfe03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot and interpret PCs\n",
    "def visualize_pcs(pca, X, label):\n",
    "    components = pca.components_\n",
    "    features = X.columns\n",
    "    num_pcs = components.shape[0]\n",
    "    \n",
    "    # Visualize PCs\n",
    "    for i in range(num_pcs):\n",
    "        plt.bar(features, components[i], label=f'PC{i + 1}')\n",
    "        plt.title(f'{label} - PC{i + 1}')\n",
    "        plt.ylabel('Contribution')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n",
    "    \n",
    "    return components\n",
    "\n",
    "# Visualize and interpret PCs\n",
    "print(\"Xstd_df PCs:\")\n",
    "pcs_std = visualize_pcs(pca_std, Xstd_df, label=\"Xstd_df\")\n",
    "print(\"\\nXmm_df PCs:\")\n",
    "pcs_mm = visualize_pcs(pca_mm, Xmm_df, label=\"Xmm_df\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c8f0b8-37c5-403a-93ad-8abefd96e3b2",
   "metadata": {},
   "source": [
    "## Exercise 4. $k$-Means\n",
    "\n",
    "In the cells below, do the following operations:\n",
    "1. For each one of the two datasets (_std_ and _mm_), run the $k$-Means for clustering the data. In particular, **use the silohuette score for identify the best value for $k\\in\\{3, \\ldots, 10\\}$**.\n",
    "2. Plot the score graphs of exercise 3.3, adding the centroids of the cluster.\n",
    "3. Observing the centroids coordinates in the PC space, **give a name/interpretation to them**, exploiting the names you assigned to the PCs. **Comment and motivate your interpretations**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2e7be2-1756-4e44-9bcd-56a0fa7c78b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c165c3e8-1dbb-46c0-b09e-d7704464c324",
   "metadata": {},
   "source": [
    "## Exercise 5. Cluster Evaluations\n",
    "\n",
    "In the cells below, do the following operations:\n",
    "1. For each one of the two datasets (_std_ and _mm_), perform an **external evaluation** of the clustering obtained at exercise 4.1 with respect to one or more labels in the list _labels_. **Comment the results, comparing the evaluation with the interpretation you gave at exercise 4.3**. \n",
    "2. For each one of the two datasets (_std_ and _mm_), perform an **internal evaluation** of each cluster, with respect to the silohuette score. **Comment the results**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4babe2f4-f800-4f51-9c97-6059d4ca12cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
